---
title: "case_study_report"
author: "Connie Wu, Jason McEachin, Joe Choo, Scott Heng"
date: "10/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(dplyr)
library(tidyverse)
library(corrplot)
library(gridExtra)
library(reshape2)
library(MLmetrics)
```

```{r, include=FALSE}
test <- read.csv(file='data-test.csv')
train <- read.csv(file='data-train.csv')
```

<!-- EDA-->

```{r}
#pairs plot to show correlation between response variables
train_response <- data.frame(train[,c("R_moment_1","R_moment_2","R_moment_3","R_moment_4")])
pairs(train_response)
```

```{r, echo=FALSE, warning= FALSE, fig.width=13, fig.height=3.5, fig.align='center'}
#plots of each predictor variable with moment 1 (can be changed to other moments)
st_m1 <- ggplot(train,aes(x=St, y=R_moment_1)) + geom_point()
re_m1 <- ggplot(train,aes(x=Re, y=R_moment_1)) + geom_point()
fr_m1 <- ggplot(train,aes(x=Fr, y=R_moment_1)) + geom_point()

gridExtra::grid.arrange(st_m1,re_m1,fr_m1, ncol=3)
```

```{r}
#correlation between predictor variables (in case we need to include interaction effects)
train_predictors <- data.frame(train[,c("Re","Fr","St")])
pairs(train_predictors)
```

<!-- Modelling-->

```{r, include=FALSE}
#linear regression for each response variable
train_noinf <- data.frame(train)
train_noinf[train_noinf == Inf] <- 0
lm_m1 <- lm(R_moment_1 ~ St + Fr + Re, data=train_noinf)
lm_m2 <- lm(R_moment_2 ~ St + Fr + Re, data=train_noinf)
lm_m3 <- lm(R_moment_3 ~ St + Fr + Re, data=train_noinf)
lm_m4 <- lm(R_moment_4 ~ St + Fr + Re, data=train_noinf)

summary(lm_m1)
summary(lm_m2)
summary(lm_m3)
summary(lm_m4)
```

```{r, include=FALSE}
#linear regression (continued)- predictions
test[test==Inf] <- 0
p1 <- predict.lm(lm_m1, data=test)
p2 <- predict.lm(lm_m2, data=test)
p3 <- predict.lm(lm_m3, data=test)
p4 <- predict.lm(lm_m4, data=test)
```

```{r, include=FALSE}
#linear regression (continued) -interaction effects
lm_m1_int <- lm(R_moment_1 ~ St + Fr + Re + St*Fr + St*Re + Fr*Re, data=train_noinf)
lm_m2_int <- lm(R_moment_2 ~ St + Fr + Re + St*Fr + St*Re + Fr*Re, data=train_noinf)
lm_m3_int <- lm(R_moment_3 ~ St + Fr + Re + St*Fr + St*Re + Fr*Re, data=train_noinf)
lm_m4_int <- lm(R_moment_4 ~ St + Fr + Re + St*Fr + St*Re + Fr*Re, data=train_noinf)

summary(lm_m1_int)
summary(lm_m2_int)
summary(lm_m3_int)
summary(lm_m4_int)
```

```{r}
#linear regression with factored Re and Fr
train1 <- train
train1$Re <- as.factor(train$Re)
train1$Fr <- as.factor(train$Fr)
lm1_m1 <- lm(R_moment_1 ~ St + Fr + Re, data=train1)
lm2_m1 <- lm(R_moment_2 ~ St + Fr + Re, data=train1)
lm3_m1 <- lm(R_moment_3 ~ St + Fr + Re, data=train1)
lm4_m1 <- lm(R_moment_4 ~ St + Fr + Re, data=train1)
summary(lm1_m1)
```




```{r moment1 ridge}
x <- model.matrix(R_moment_1~St + Fr + Re,data=train_noinf)[,-1]
y <- train_noinf$R_moment_1 

library(glmnet)
grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)


train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2) ## calculate MSE
```


```{r}

set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

```

```{r ridge with best lam}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)

```
improvment in the mse


```{r moment2 ridge}
x2 <- model.matrix(R_moment_2~ St + Fr + Re,data=train_noinf)[,-1]
y2 <- train_noinf$R_moment_2 

library(glmnet)
set.seed(1)
grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param
ridge.mod2 <- glmnet(x2, y2, alpha = 0, lambda = grid)


train2 <- sample(1:nrow(x2), nrow(x2)/2)
test2 <- (-train2)
y.test2 <- y2[test2]

ridge.mod2 <- glmnet(x2[train,], y2[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x2[test,])
mean((ridge.pred - y.test2)^2) ## calculate MSE
```


```{r best lam2}

set.seed(1)
cv.out2 <- cv.glmnet(x2[train2,], y2[train2], alpha = 0)
plot(cv.out2)
bestlam2 <- cv.out2$lambda.min
bestlam2

```

```{r ridge2 with best lam}
ridge.pred2 <- predict(ridge.mod2, s = bestlam2, newx = x2[test2,])
mean((ridge.pred2 - y.test2)^2)
```



```{r ridge model 3 and 4}

x3 <- model.matrix(R_moment_3~St + Fr + Re,data=train_noinf)[,-1]
y3 <- train_noinf$R_moment_3


ridge.mod3 <- glmnet(x3, y3, alpha = 0, lambda = grid)


train3 <- sample(1:nrow(x3), nrow(x3)/2)
test3 <- (-train3)
y.test3 <- y[test3]

ridge.mod3 <- glmnet(x3[train3,], y3[train3], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred3 <- predict(ridge.mod3, s = 4, newx = x3[test3,])
mean((ridge.pred3 - y.test3)^2) ## calculate MSE


x4 <- model.matrix(R_moment_4~St + Fr + Re,data=train_noinf)[,-1]
y4 <- train_noinf$R_moment_4

ridge.mod4 <- glmnet(x4, y4, alpha = 0, lambda = grid)


train4 <- sample(1:nrow(x4), nrow(x4)/2)
test4 <- (-train4)
y.test4 <- y[test4]

ridge.mod4 <- glmnet(x4[train4,], y4[train4], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred4 <- predict(ridge.mod4, s = 4, newx = x4[test4,])
mean((ridge.pred4 - y.test4)^2) ## calculate MSE
```
```{r best lam3 and lam4}

set.seed(1)
cv.out3 <- cv.glmnet(x3[train3,], y3[train3], alpha = 0)
plot(cv.out3)
bestlam3 <- cv.out3$lambda.min
bestlam3


cv.out4 <- cv.glmnet(x4[train4,], y4[train4], alpha = 0)
plot(cv.out4)
bestlam4 <- cv.out4$lambda.min
bestlam4
```
```{r ridge3 and ridge4 with best lam}
ridge.pred3 <- predict(ridge.mod3, s = bestlam3, newx = x3[test3,])
mean((ridge.pred3 - y.test3)^2)

ridge.pred4 <- predict(ridge.mod4, s = bestlam4, newx = x4[test4,])
mean((ridge.pred4 - y.test4)^2)

```
improvement? still large

<!-- Paper -->

# Introduction
<!-- outline key research objectives and how your model achieves such objectives -->

Turbulence is  highly versatile motion that is often times difficult to predict and understanding
fluid motion and its effect on natural problems poses a great challenge.
Interpreting turbulence data is an incredibly important task in the engineering
from understanding the cosmos to the cosmic cycle. 

Parametric modeling is effective when we want to compactly represent features as model
parameters. Unlike certain “black box” machine learning techniques, it offers a 
higher level of interpretability, which is especially useful for a practical 
setting.Rather than simply outputting a classification result, a parametric 
model allows us to investigate in more detail which aspects of turbulence 
differ between high and low particle cluster volumes.

In our project, we use linear and (...)  and apply it to interpret
the difference in model parameters in order to achieve the the following objectives:

1. Build a model that predict its particle cluster volume distribution in terms 
of the moments.

2. Investigate and interpret how model parameters affects the probability 
distribution for particle cluster volumes

# Methodology
<!-- describes your statistical model, how your model is fit from data, and justifying why your model is appropriate given the problem or dataset -->



# Results
<!-- discussing your predictive results (don't forget uncertainty!), as well as insights on the scientific problem. You should also submit your predictions on the hold-out set in data-test.csv, in the form of a .csv file-->

# Conclusion
<!-- summarizing key findings of your study -->

