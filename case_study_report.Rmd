---
title: "case_study_report"
author: "Connie Wu, Jason McEachin, Joe Choo, Scott Heng"
date: "10/10/2020"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(dplyr)
library(tidyverse)
library(corrplot)
library(gridExtra)
library(reshape2)
library(MLmetrics)
library(boot)
library(splines)
library(glmnet)
```

```{r, include=FALSE}
test <- read.csv(file='data-test.csv')
train <- read.csv(file='data-train.csv')
train$Fr.logit <- inv.logit(train$Fr)
test$Fr.logit <- inv.logit(test$Fr)
```

<!-- Paper -->

# Introduction
<!-- outline key research objectives and how your model achieves such objectives -->

Turbulence is  highly versatile motion that is often times difficult to predict and understanding
fluid motion and its effect on natural problems poses a great challenge.
Interpreting turbulence data is an incredibly important task in the engineering
from understanding the cosmos to the cosmic cycle. 

Parametric modeling is effective when we want to compactly represent features as model
parameters. Unlike certain “black box” machine learning techniques, it offers a 
higher level of interpretability, which is especially useful for a practical 
setting.Rather than simply outputting a classification result, a parametric 
model allows us to investigate in more detail which aspects of turbulence 
differ between high and low particle cluster volumes.

In our project, we use linear and (...)  and apply it to interpret
the difference in model parameters in order to achieve the the following objectives:

1. Build a model that predict its particle cluster volume distribution in terms 
of the moments.

2. Investigate and interpret how model parameters affects the probability 
distribution for particle cluster volumes

# Methodology
<!-- describes your statistical model, how your model is fit from data, and justifying why your model is appropriate given the problem or dataset -->



# Results
<!-- discussing your predictive results (don't forget uncertainty!), as well as insights on the scientific problem. You should also submit your predictions on the hold-out set in data-test.csv, in the form of a .csv file-->

# Conclusion
<!-- summarizing key findings of your study -->

# Appendix

### EDA 

```{r}
#pairs plot to show correlation between response variables
train_response <- data.frame(train[,c("R_moment_1","R_moment_2","R_moment_3","R_moment_4")])
pairs(train_response)
```

```{r, echo=FALSE, warning= FALSE, fig.width=13, fig.height=3.5, fig.align='center'}
#plots of each predictor variable with moment 1 (can be changed to other moments)
st_m1 <- ggplot(train,aes(x=St, y=R_moment_1)) + geom_point()
re_m1 <- ggplot(train,aes(x=Re, y=R_moment_1)) + geom_point()
fr_m1 <- ggplot(train,aes(x=Fr, y=R_moment_1)) + geom_point()

gridExtra::grid.arrange(st_m1,re_m1,fr_m1, ncol=3)
```

```{r}
#correlation between predictor variables (in case we need to include interaction effects)
train_predictors <- data.frame(train[,c("Re","Fr","St")])
pairs(train_predictors)
```

### Modeling

#### Simple linear modeling
```{r, include=FALSE}
#linear regression for each response variable
lm_m1 <- lm(R_moment_1 ~ St + Fr.logit + Re, data=train)
lm_m2 <- lm(R_moment_2 ~ St + Fr.logit + Re, data=train)
lm_m3 <- lm(R_moment_3 ~ St + Fr.logit + Re, data=train)
lm_m4 <- lm(R_moment_4 ~ St + Fr.logit + Re, data=train)

summary(lm_m1)
summary(lm_m2)
summary(lm_m3)
summary(lm_m4)
```

```{r, include=FALSE}
#linear regression (continued)- predictions
p1 <- predict.lm(lm_m1, data=test)
p2 <- predict.lm(lm_m2, data=test)
p3 <- predict.lm(lm_m3, data=test)
p4 <- predict.lm(lm_m4, data=test)
```

#### Interaction effects

```{r, include=FALSE}
#linear regression (continued) -interaction effects
lm_m1_int <- lm(R_moment_1 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)
lm_m2_int <- lm(R_moment_2 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)
lm_m3_int <- lm(R_moment_3 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)
lm_m4_int <- lm(R_moment_4 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)

summary(lm_m1_int)
summary(lm_m2_int)
summary(lm_m3_int)
summary(lm_m4_int)
```

```{r}
x <- model.matrix(R_moment_1~St + Fr.logit + Re,data=train)[,-1]

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_1[test]

preds <- predict(lm_m1_int, newdata = as.data.frame(train[test,]))
mean((preds - y.test)^2)
```

#### Predictors as factors

```{r}
#linear regression with factored Re and Fr
train1 <- train
train1$Re <- as.factor(train$Re)
train1$Fr.logit <- as.factor(train$Fr.logit)
lm1_m1 <- lm(R_moment_1 ~ St + Fr.logit + Re, data=train1)
lm2_m1 <- lm(R_moment_2 ~ St + Fr.logit + Re, data=train1)
lm3_m1 <- lm(R_moment_3 ~ St + Fr.logit + Re, data=train1)
lm4_m1 <- lm(R_moment_4 ~ St + Fr.logit + Re, data=train1)
summary(lm1_m1)

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_1[test]

preds <- predict(lm1_m1, newdata = as.data.frame(train1[test,]))
mean((preds - y.test)^2)
```

#### Ridge Regression 

```{r moment1 ridge}
x <- model.matrix(R_moment_1~St + Fr.logit + Re,data=train)[,-1]
y <- train$R_moment_1 

set.seed(17)
train.samp <- sample(1:nrow(x), nrow(x)/2)
test.samp <- (-train.samp)
y.test <- y[test.samp]

grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param

ridge.mod <- glmnet(x[train.samp,], y[train.samp], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s=0, x = x[train.samp,], y = y[train.samp],
                      newx = x[test.samp,], exact = T)
mean((ridge.pred - y.test)^2) ## calculate MSE
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train.samp,], y[train.samp], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

```{r ridge with best lam}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test.samp,])
mean((ridge.pred - y.test)^2)
```
MSE stays basically the same

```{r moment2 ridge}
x2 <- model.matrix(R_moment_2~ St + Fr + Re + poly(St,2) + poly(Fr,2) + poly(Re,2) + St:Re,data=train_noinf)[,-1]
y2 <- train_noinf$R_moment_2 

set.seed(17)
train2 <- sample(1:nrow(x2), 4 * nrow(x2)/5)
test2 <- (-train2)
y.test2 <- y2[test2]

grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param

ridge.mod2 <- glmnet(x2[train2,], y2[train2], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred2 <- predict(ridge.mod2, s=0, x = x2[train2,], y = y2[train2],
                      newx = x2[test2,], exact = T)
mean((ridge.pred2 - y.test2)^2) ## calculate MSE
```

```{r best lam2}
set.seed(1)
cv.out2 <- cv.glmnet(x2[train2,], y2[train2], alpha = 0)
plot(cv.out2)
bestlam2 <- cv.out2$lambda.min
bestlam2
```

```{r ridge2 with best lam}
ridge.pred2 <- predict(ridge.mod2, s = bestlam2, newx = x2[test2,])
mean((ridge.pred2 - y.test2)^2)
```

```{r ridge model 3}
x3 <- model.matrix(R_moment_3~St + Fr + Re,data=train_noinf)[,-1]
y3 <- train_noinf$R_moment_3

set.seed(17)
train3 <- sample(1:nrow(x3), nrow(x3)/2)
test3 <- (-train3)
y.test3 <- y3[test3]

ridge.mod3 <- glmnet(x3[train3,], y3[train3], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred3 <- predict(ridge.mod3, s = 0, newx = x3[test3,])
mean((ridge.pred3 - y.test3)^2) ## calculate MSE
```

```{r best lam3}
set.seed(1)
cv.out3 <- cv.glmnet(x3[train3,], y3[train3], alpha = 0)
plot(cv.out3)
bestlam3 <- cv.out3$lambda.min
bestlam3
```

```{r ridge3 with best lam}
ridge.pred3 <- predict(ridge.mod3, s = bestlam3, newx = x3[test3,])
mean((ridge.pred3 - y.test3)^2)
```

```{r ridge model 4}
x4 <- model.matrix(R_moment_4~as.factor(St) + as.factor(Fr) + as.factor(Re),data=train_noinf)[,-1]
y4 <- train_noinf$R_moment_4

set.seed(17)
train4 <- sample(1:nrow(x4), nrow(x4)/2)
test4 <- (-train4)
y.test4 <- y[test4]

ridge.mod4 <- glmnet(x4[train4,], y4[train4], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred4 <- predict(ridge.mod4, s = 0, newx = x4[test4,])
mean((ridge.pred4 - y.test4)^2) ## calculate MSE
```

```{r best lam4}
set.seed(1)
cv.out4 <- cv.glmnet(x4[train4,], y4[train4], alpha = 0)
plot(cv.out4)
bestlam4 <- cv.out4$lambda.min
bestlam4
```

```{r ridge4 with best lam}
ridge.pred4 <- predict(ridge.mod4, s = bestlam4, newx = x4[test4,])
mean((ridge.pred4 - y.test4)^2)
```
improvement? still large

#### GAMS
```{r first moment gam}
x <- model.matrix(R_moment_1~St + Fr.logit + Re,data=train)[,-1]

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_1[test]

gam1 <- lm(R_moment_1 ~ ns(St, 1) + ns(Re, 1) + ns(Fr.logit, 1), data = train, subset = train.samp)
preds <- predict(gam1, newdata = as.data.frame(x[test,]))
mean((preds - y.test)^2)
```

```{r second moment gam}
x <- model.matrix(R_moment_2 ~ St + Fr.logit +Re,data=train)[,-1]

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_2[test]

gam2 <- lm(R_moment_2 ~ ns(St, 2) + ns(Re, 2) + ns(Fr.logit, 2), data = train, subset = train.samp)
preds <- predict(gam2, newdata = as.data.frame(x[test,]))
mean((preds - y.test)^2)
```
