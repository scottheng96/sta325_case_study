---
title: "case_study_report"
author: "Connie Wu, Jason McEachin, Joe Choo, Scott Heng"
date: "10/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(dplyr)
library(tidyverse)
library(corrplot)
library(gridExtra)
library(reshape2)
library(MLmetrics)
library(boot)
library(splines)
library(glmnet)
library(kableExtra)
library(caret)
library(psych)
```

```{r, include=FALSE}
test <- read.csv(file='data-test.csv')
train <- read.csv(file='data-train.csv')
train$Fr.logit <- inv.logit(train$Fr)
test$Fr.logit <- inv.logit(test$Fr)

train1 <- train
train1$Re <- as.factor(train$Re)
train1$Fr.logit <- as.factor(train$Fr.logit)
```

<!-- Paper -->

# Introduction
<!-- outline key research objectives and how your model achieves such objectives -->

## Background Information

Turbulence is  highly versatile motion that is often times difficult to predict and understanding
fluid motion and its effect on natural problems poses a great challenge.
Interpreting turbulence data is an incredibly important task in the engineering
from understanding the cosmos to the cosmic cycle. 

Parametric modeling is effective when we want to compactly represent features as model
parameters. Unlike certain “black box” machine learning techniques, it offers a 
higher level of interpretability, which is especially useful for a practical 
setting.Rather than simply outputting a classification result, a parametric 
model allows us to investigate in more detail which aspects of turbulence 
differ between high and low particle cluster volumes.

In our project, we use linear and (...)  and apply it to interpret
the difference in model parameters in order to achieve the the following objectives:

1. Build a model that predict its particle cluster volume distribution in terms 
of the moments.

2. Investigate and interpret how model parameters affects the probability 
distribution for particle cluster volumes

## Data

The data set procured for this case study consists of information about cluster volumes. In total, it contains 89 observations with 7 variables. Details of each variable is specified in Table 1.1. The original response variable, a probability distribution for particle cluster volumes, is difficult to interpret, and therefore is summarized into its first 4 raw moments.

```{r, echo=FALSE, warning=FALSE}
features_tbl <- data.frame(
  Metric = c(rep("St", 1),
             rep("Re", 1),
             rep("Fr",1),
             rep("R moment 1",1),
             rep("R moment 2",1),
             rep("R moment 3",1),
             rep("R moment 4",1)),
  
  Value = c("0 < St < 3",
            "90, 224, 398",
            "Infinity, 0, 3",
            "Continuous response variable",
            "Continuous response variable",
            "Continuous response variable",
            "Continuous response variable"),
    Description = linebreak(c("Particle property: effect on inertia (e.g. size, density)",
                       "Reynolds Number: turbulent flow property",
                       "Particle propety: gravitational acceleration",
                       "First raw moment of probability distribution",
                       "Second raw moment of volume probability distribution",
                       "Third raw moment of volume probability distribution",
                       "Fourth raw moment of volume probability distribution")
  )
)
features_tbl %>% 
  kable(escape=F,   booktabs = T, align = "c",
        caption = "Table 1.1 Description Table of Data set variables") %>%
  kable_styling(latex_options = c("hold_position")) %>% 
  collapse_rows(columns = 1, latex_hline = "major") %>% 
  row_spec(0, bold = TRUE) #%>% 
#column_spec(column = c(2), latex_column_spec = "c")
```

Performing some exploratory data analysis, we can observe from Figure 1 that $St$, with the exception of the 0 values, follow a linear relationship with the first moment, while for $Re$, there is high variability for the first moment when $Re$ = 90, and low variability otherwise. There is high variability across all $Fr$ values with respect to the first moment. Some of these insights influenced our decisions for modelling approaches which will be further discussed in the Methods section.

```{r, echo=FALSE, warning= FALSE, fig.width=13, fig.height=3.5, fig.align='center', fig.cap="Exploratory data analysis plots on predictors vs first moment"}
#plots of each predictor variable with moment 1 (can be changed to other moments)
st_m1 <- ggplot(train,aes(x=St, y=R_moment_1)) + geom_point()
re_m1 <- ggplot(train,aes(x=Re, y=R_moment_1)) + geom_point()
fr_m1 <- ggplot(train,aes(x=Fr, y=R_moment_1)) + geom_point()
gridExtra::grid.arrange(st_m1,re_m1,fr_m1, ncol=3)
```

Finally, since `Fr` contains values of infinity, we decided to inverse-logit the variable, thus creating a new variable called `Fr.logit`. We use this new variable in our models so as to decrease the originally broad range of `Fr` and improve our predictions.

# Methodology
<!-- describes your statistical model, how your model is fit from data, and justifying why your model is appropriate given the problem or dataset -->
We fit a linear regression model with interaction effects, taking $Re$ and $Fr.logit$ as factor variables and $St$ as a bounded, continuous variable. Other models were also explored, but they had larger limitations in comparison with the linear model which is discussed further in the report. We implement a linear regression model for each moment, with a total of 4 models for 4 moments.

For our chosen linear regression model, we can write this in statistical notation below:

$$
\begin{aligned}
Moment_x = \beta_0 + \beta_1*St + \sum_{i=2}^{4}\beta_i*Re_i + \sum_{j=5}^{7}\beta_j*Fr.logit_j \\ + \sum_{k=8}^{10}\beta_k *\text{(all two-way interactions between St, Re and Fr.logit)} \\
\text{For }   1 \leq x \leq 4, \\
\end{aligned}
$$

Considering the nature of our response variables, since each moment is derived from the same probability distribution, we decided to perform log transformations for the higher-order moments ($Moment_2$, $Moment_3$ and $Moment_4$). Furthmore, our design decision to include interaction effects stem from the exploratory data plot showing certain correlations between the predictor variables, so as to allow our model to be more interpretable to the collinearity between pair of predictors. It is important to note that the baseline level for the now categorical variable `Fr.logit` is 0.51, whereas the baseline level for the categorical variable `Re` is 90 in all of our models.

# Results
<!-- discussing your predictive results (don't forget uncertainty!), as well as insights on the scientific problem. You should also submit your predictions on the hold-out set in data-test.csv, in the form of a .csv file-->

For the model implemented with the log of the second moment as the response variable, we see that all predictor variables and the interaction effects between Fr and Re to be significant (p-values <0.05). A high adjusted R-squared value of 0.889 shows that the model explains a large amount of variability in its predictions considering multiple predictors and their collinearity. There Residual Mean-squared Error is 1.237, which is low, meaning that the model is fit well. Holding all else constant, a unit increase in St results in a e^(0.8586) increase of the second moment. When Fr is 0.574 and 1, there is a e^(-6.678) and e^(-6.737) change of the second moment respectively. Similar when Re is 224 and 398, there is a e^(-7.43451) and e^(-10.7873) change of the second moment respectively. Additionally, when Fr is 0.574 and Re is 224 and 398, there is a e^(4.477) change in the second moment. When Fr is 1, and Re is 224 and 398, there an additional e^(4.694) and e^(6.883) change in the second moment.

```{r mom3 final, eval = F}
set.seed(1)

data_ctrl <- trainControl(method = "cv", number = 5)
model_caret3 <- train(log(R_moment_3) ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1,                       
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)

model_caret3
summary(model_caret3$finalModel)
```

For the third moment, we can see that 4 of the interaction terms, the different combinations of `St:Fr.logit` and `St:Re`, are no longer significant compared to their baseline levels, although their main effects are still significant at an alpha level of 0.05. Holding all else constant, 

```{r mom4 final, eval = F}
set.seed(1)

data_ctrl <- trainControl(method = "cv", number = 5)
model_caret4 <- train(log(R_moment_4) ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1,                      
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)

summary(model_caret4)
```

For the fourth moment, we can similarly see that the interaction terms including St are no longer significant, while their main effects are still significant. The Residual standard error is 2.693 which is higher than other models, but is expected since moment 4 is a higher-order derivation of the probability distribution compared to moments 1,2 and 3. However, a high adjusted R-squared value of 0.8784 tells us that the model explains much of the variability in the predictive capabilities of the model, but is significant lower than the other models with lower-order moments. Holding all else constant, a unit increase in `St` causes a e^(1.7147) increase in the 4th moment. When `Fr`= 0.5744 and 1, there is an e^(0.3206) and e^(0.0065) increase in the 4th moment respectively. When `Re` = 224 and 398, there is an e^(0.1384) and e^(-1.3817) increase in the 4th moment respectively. Additionally, when `Fr`=0.5744, and `Re` = 224, there is an additional e^(12.435) increase in the 4th moment. When `Fr` = `, and `Re` = 224 and 398, there is an additional e^(12.719) and e^(19.134) increase in the 4th moment respectively. 

# Conclusion
<!-- summarizing key findings of your study -->


# Appendix

### EDA 

```{r}
#pairs plot to show correlation between response variables
train_response <- data.frame(train[,c("R_moment_1","R_moment_2","R_moment_3","R_moment_4")])
pairs(train_response)
```

```{r, echo=FALSE, warning= FALSE, fig.width=13, fig.height=3.5, fig.align='center'}
#plots of each predictor variable with moment 1 (can be changed to other moments)
st_m1 <- ggplot(train,aes(x=St, y=R_moment_1)) + geom_point()
re_m1 <- ggplot(train,aes(x=Re, y=R_moment_1)) + geom_point()
fr_m1 <- ggplot(train,aes(x=Fr, y=R_moment_1)) + geom_point()

gridExtra::grid.arrange(st_m1,re_m1,fr_m1, ncol=3)
```

```{r}
#correlation between predictor variables (in case we need to include interaction effects)
train_predictors <- data.frame(train[,c("Re","Fr","St")])
pairs(train_predictors)
```

### Modeling

#### Simple linear modeling
```{r, include=FALSE}
#linear regression for each response variable
lm_m1 <- lm(R_moment_1 ~ St + Fr.logit + Re, data=train)
lm_m2 <- lm(R_moment_2 ~ St + Fr.logit + Re, data=train)
lm_m3 <- lm(R_moment_3 ~ St + Fr.logit + Re, data=train)
lm_m4 <- lm(R_moment_4 ~ St + Fr.logit + Re, data=train)

summary(lm_m1)
summary(lm_m2)
summary(lm_m3)
summary(lm_m4)
```

```{r, include=FALSE}
#linear regression (continued)- predictions
p1 <- predict.lm(lm_m1, data=test)
p2 <- predict.lm(lm_m2, data=test)
p3 <- predict.lm(lm_m3, data=test)
p4 <- predict.lm(lm_m4, data=test)
```

```{r lm cross validation}
data_ctrl <- trainControl(method = "cv", number = 5)
model_caret <- train(R_moment_1 ~ St + Fr.logit + Re, data=train,                       
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)

model_caret
plot(model_caret$finalModel)
```






#### Interaction effects

```{r, include=FALSE}
#linear regression (continued) -interaction effects
lm_m1_int <- lm(R_moment_1 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)
lm_m2_int <- lm(R_moment_2 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)
lm_m3_int <- lm(R_moment_3 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)
lm_m4_int <- lm(R_moment_4 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train)

summary(lm_m1_int)
summary(lm_m2_int)
summary(lm_m3_int)
summary(lm_m4_int)
```

```{r}
x <- model.matrix(R_moment_1~St + Fr.logit + Re,data=train)[,-1]

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_1[test]

preds <- predict(lm_m1_int, newdata = as.data.frame(train[test,]))
mean((preds - y.test)^2)
```

```{r interaction cross validation}
data_ctrl <- trainControl(method = "cv", number = 5)
model_caret <- train(R_moment_1 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train,                       
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)

summary(model_caret)
```



#### Predictors as factors

```{r}
#linear regression with factored Re and Fr.logit
train1 <- train
train1$Re <- as.factor(train$Re)
train1$Fr.logit <- as.factor(train$Fr.logit)
lm1_m1 <- lm(R_moment_1 ~ St + Fr.logit + Re, data=train1)
lm2_m1 <- lm(R_moment_2 ~ St + Fr.logit + Re, data=train1)
lm3_m1 <- lm(R_moment_3 ~ St + Fr.logit + Re, data=train1)
lm4_m1 <- lm(R_moment_4 ~ St + Fr.logit + Re, data=train1)

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_1[test]

preds <- predict(lm1_m1, newdata = as.data.frame(train1[test,]))
mean((preds - y.test)^2)

lm1_m1_int <- lm(R_moment_1 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1)
lm2_m1_int <- lm(R_moment_2 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1)
lm3_m1_int <- lm(R_moment_3 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1)
lm4_m1_int <- lm(R_moment_4 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1)
```

```{r as factor cross validation}
set.seed(1)
data_ctrl <- trainControl(method = "cv", number = 5)
model_caret <- train(R_moment_1 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1,                       
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)

model_caret
plot(model_caret$finalModel)
summary(model_caret$finalModel)


model_caret_2 <- train(log(R_moment_2) ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1,                       
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)

model_caret_2
plot(model_caret_2$finalModel)
summary(model_caret_2$finalModel)

```

The model’s coefficients are all negative except the St and Fr.logit:Re 
coefficients, which align with our prior beliefs. A larger particle size could 
increase the probability distribution for particle cluster volumes. Further, 
the coefficients of significance are St, Re and their interaction.

The R-squared value of 0.6883 shows  the factors in the model successfully 
explain the most of the variance.



#### Ridge Regression 

```{r moment1 ridge}
x <- model.matrix(R_moment_1~St + Fr.logit + Re,data=train)[,-1]
y <- train$R_moment_1 

set.seed(17)
train.samp <- sample(1:nrow(x), nrow(x)/2)
test.samp <- (-train.samp)
y.test <- y[test.samp]

grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param

ridge.mod <- glmnet(x[train.samp,], y[train.samp], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s=0, x = x[train.samp,], y = y[train.samp],
                      newx = x[test.samp,], exact = T)
mean((ridge.pred - y.test)^2) ## calculate MSE
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train.samp,], y[train.samp], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

```{r ridge with best lam}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test.samp,])
mean((ridge.pred - y.test)^2)
```
MSE stays basically the same

```{r moment2 ridge}
x2 <- model.matrix(R_moment_2~ St + Fr.logit + Re + poly(St,2) + poly(Fr.logit,2) + poly(Re,2) + St:Re,data=train)[,-1]
y2 <- train$R_moment_2 

set.seed(17)
train2 <- sample(1:nrow(x2), 4 * nrow(x2)/5)
test2 <- (-train2)
y.test2 <- y2[test2]

grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param

ridge.mod2 <- glmnet(x2[train2,], y2[train2], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred2 <- predict(ridge.mod2, s=0, x = x2[train2,], y = y2[train2],
                      newx = x2[test2,], exact = T)
mean((ridge.pred2 - y.test2)^2) ## calculate MSE
```

```{r best lam2}
set.seed(1)
cv.out2 <- cv.glmnet(x2[train2,], y2[train2], alpha = 0)
plot(cv.out2)
bestlam2 <- cv.out2$lambda.min
bestlam2
```

```{r ridge2 with best lam}
ridge.pred2 <- predict(ridge.mod2, s = bestlam2, newx = x2[test2,])
mean((ridge.pred2 - y.test2)^2)
```

```{r ridge model 3}
x3 <- model.matrix(R_moment_3~St + Fr.logit + Re,data=train)[,-1]
y3 <- train$R_moment_3

set.seed(17)
train3 <- sample(1:nrow(x3), nrow(x3)/2)
test3 <- (-train3)
y.test3 <- y3[test3]

ridge.mod3 <- glmnet(x3[train3,], y3[train3], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred3 <- predict(ridge.mod3, s = 0, newx = x3[test3,])
mean((ridge.pred3 - y.test3)^2) ## calculate MSE
```

```{r best lam3}
set.seed(1)
cv.out3 <- cv.glmnet(x3[train3,], y3[train3], alpha = 0)
plot(cv.out3)
bestlam3 <- cv.out3$lambda.min
bestlam3
```

```{r ridge3 with best lam}
ridge.pred3 <- predict(ridge.mod3, s = bestlam3, newx = x3[test3,])
mean((ridge.pred3 - y.test3)^2)
```

```{r ridge model 4}
x4 <- model.matrix(R_moment_4~as.factor(St) + as.factor(Fr.logit) + as.factor(Re),data=train)[,-1]
y4 <- train$R_moment_4

set.seed(17)
train4 <- sample(1:nrow(x4), nrow(x4)/2)
test4 <- (-train4)
y.test4 <- y[test4]

ridge.mod4 <- glmnet(x4[train4,], y4[train4], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred4 <- predict(ridge.mod4, s = 0, newx = x4[test4,])
mean((ridge.pred4 - y.test4)^2) ## calculate MSE
```

```{r best lam4}
set.seed(1)
cv.out4 <- cv.glmnet(x4[train4,], y4[train4], alpha = 0)
plot(cv.out4)
bestlam4 <- cv.out4$lambda.min
bestlam4
```

```{r ridge4 with best lam}
ridge.pred4 <- predict(ridge.mod4, s = bestlam4, newx = x4[test4,])
mean((ridge.pred4 - y.test4)^2)
```
improvement? still large

#### GAMS
```{r first moment gam}
x <- model.matrix(R_moment_1~St + Fr.logit + Re,data=train)[,-1]

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_1[test]

gam1 <- lm(R_moment_1 ~ ns(St, 1) + ns(Re, 1) + ns(Fr.logit, 1), data = train, subset = train.samp)
preds <- predict(gam1, newdata = as.data.frame(x[test,]))
mean((preds - y.test)^2)
```

```{r second moment gam}
x <- model.matrix(R_moment_2 ~ St + Fr.logit +Re,data=train)[,-1]

set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_2[test]

gam2 <- lm(R_moment_2 ~ ns(St, 2) + ns(Re, 2) + ns(Fr.logit, 2), data = train, subset = train.samp)
preds <- predict(gam2, newdata = as.data.frame(x[test,]))
mean((preds - y.test)^2)
```

#### Fourth moment

```{r fourth moment gam}
set.seed(17)
train.samp <- sample(1:nrow(train), 4 * nrow(train)/5)
test <- (-train.samp)
y.test <- train$R_moment_4[test]

gam4 <- lm(R_moment_4 ~ ns(St, 2) + ns(Re, 2) + ns(Fr.logit, 2), data = train, subset = train.samp)
preds <- predict(gam4, newdata = as.data.frame(train[test,]))
mean((preds - y.test)^2)
```

```{r}
lm4_m1_int <- lm(R_moment_4 ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1)
```

```{r as factor cross validation}
set.seed(1)

data_ctrl <- trainControl(method = "cv", number = 5)
model_caret <- train(log(R_moment_3) ~ St + Fr.logit + Re + St*Fr.logit + St*Re + Fr.logit*Re, data=train1,                       
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)

model_caret
# plot(model_caret$finalModel)
# summary(model_caret$finalModel)
```

